{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-qa-albert-pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjPKR0WhQ/j9WoTEuRxnHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_qa_albert_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "1. [AllenNLP Q&A with heatmap](https://demo.allennlp.org/reading-comprehension/MjE0MTE2MQ==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "dbde61d8-558e-48f7-aa1f-f0120dbbace5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!uname -a\n",
        "!apt-get install file"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 948adb98e3a7 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 1s (236 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "outputId": "8ee87976-30ce-45c7-8cde-581e8ab37d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a72e720e2be4b6547cf591b9ee5c9be194d701d7a4010f3d49d0a05cbf945fdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "b27f117a-eb6e-4717-929c-df05bc0f0851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMf33lSSGpQ",
        "colab_type": "code",
        "outputId": "33bd948b-b1fd-40f6-96a2-fde38f11cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!ls -al\n",
        "!rm -rf nlp-qa\n",
        "!git clone --depth 1 https://github.com/hailusong/nlp-qa.git"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 .\n",
            "drwxr-xr-x 1 root root 4096 Jun  4 18:40 ..\n",
            "drwxr-xr-x 1 root root 4096 Jun  2 16:14 .config\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 sample_data\n",
            "Cloning into 'nlp-qa'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "9b8ee1b1-5621-4e95-abce-fa637840ebca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -r nlp-qa/question-answering/requirements.txt"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'nlp-qa/question-answering/requirements.txt'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MULTILABEL_INTENT=True\n",
        "QA_EXTRACTION=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained ALBERT v2 language model \n",
        "https://huggingface.co/transformers/pretrained_models.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "0f945ac5-8d8c-4b4d-8d1d-e9a713dbdf1a"
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "  import torch\n",
        "  model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'albert-large-v2')    # Download model and configuration from S3 and cache.\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'albert-large-v2', output_attentions=True)  # Update configuration during loading\n",
        "  # assert model.config.output_attentions == True\n",
        "  model\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlbertModel(\n",
              "  (embeddings): AlbertEmbeddings(\n",
              "    (word_embeddings): Embedding(30000, 128, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 128)\n",
              "    (token_type_embeddings): Embedding(2, 128)\n",
              "    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0, inplace=False)\n",
              "  )\n",
              "  (encoder): AlbertTransformer(\n",
              "    (embedding_hidden_mapping_in): Linear(in_features=128, out_features=1024, bias=True)\n",
              "    (albert_layer_groups): ModuleList(\n",
              "      (0): AlbertLayerGroup(\n",
              "        (albert_layers): ModuleList(\n",
              "          (0): AlbertLayer(\n",
              "            (full_layer_layer_norm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            (attention): AlbertAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
              "            )\n",
              "            (ffn): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (ffn_output): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "  (pooler_activation): Tanh()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQw1NmYogxY",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained ALBERT v2 model on SQuAD2\n",
        "https://huggingface.co/elgeish/cs224n-squad2.0-albert-xxlarge-v1, including:\n",
        "- config.json, 721.0B\n",
        "- pytorch_model.bin, 849.2MB\n",
        "- special_tokens_map.json, 156.0B\n",
        "- spiece.model, 742.5KB\n",
        "- tokenizer_config.json, 39.0B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYVCaev5YQEt",
        "colab_type": "code",
        "outputId": "e4998f6d-0afe-47fa-d25d-f75f88fec09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  from transformers import ( AutoTokenizer, AutoModelForQuestionAnswering )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"elgeish/cs224n-squad2.0-albert-xxlarge-v1\")\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(\"elgeish/cs224n-squad2.0-albert-xxlarge-v1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmN-BLTg-Cp",
        "colab_type": "code",
        "outputId": "d2f5e869-22d7-4049-8693-2ee1ce184e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  # inspect tokenizer max length\n",
        "  print(f'Tokenizer max input length {tokenizer.model_max_length}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer max input length 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghMbXVpFKhr",
        "colab_type": "code",
        "outputId": "f114005f-72d9-43ba-eeb2-10cc41e7498f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'modelForQuestionAnswering', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "  # tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "  # The format is paragraph first and then question\n",
        "  # fact = \"Jim Henson was a puppeteer\"\n",
        "  # q = \"Who was Jim Henson ?\"\n",
        "  fact = input('Fact: ')\n",
        "  q = input('Question: ')\n",
        "\n",
        "  # Converts a list of string to a sequence of ids (integer), using the tokenizer and vocabulary. \n",
        "  # Also request to add model-specific special tokens (such as beginning of sequence, end of sequence, sequence separator).\n",
        "  indexed_tokens_fact = tokenizer.encode(fact, add_special_tokens=True)\n",
        "  indexed_tokens_q = tokenizer.encode(q, add_special_tokens=True)\n",
        "\n",
        "  # merge fact tokens and Q tokens by removing the START token in Q tokens\n",
        "  indexed_tokens = indexed_tokens_fact + indexed_tokens_q[1:]\n",
        "  segments_ids = [0] * len(indexed_tokens_fact) + [1] * len(indexed_tokens_q[1:])\n",
        "\n",
        "  print(f'Length is {len(indexed_tokens)}, Indexed token: {indexed_tokens}')\n",
        "\n",
        "  # Predict the start and end positions logits\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      start_logits, end_logits = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "  # get the highest prediction\n",
        "  answer = tokenizer.decode(indexed_tokens[torch.argmax(start_logits):torch.argmax(end_logits)+1])\n",
        "  print(f'The answer is: {answer}')\n",
        "\n",
        "  # Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n",
        "  # start_positions, end_positions = torch.tensor([12]), torch.tensor([14])\n",
        "  # multiple_choice_loss = model(tokens_tensor, token_type_ids=segments_tensors, start_positions=start_positions, end_positions=end_positions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fact: Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electrical engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies are used to develop machines that can substitute for humans. Robots can be used in any situation and for any purpose, but today many are used in dangerous environments (including bomb detection and de-activation), manufacturing processes, or where humans cannot survive. Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, and basically anything a human can do.\n",
            "Question: What do robots that resemble humans plan  to do?\n",
            "Length is 183, Indexed token: [2, 15219, 18, 25, 40, 19, 20745, 1686, 16, 1552, 17, 762, 30, 1103, 4953, 1552, 15, 4618, 1552, 15, 1428, 762, 15, 17, 654, 9, 15219, 18, 10342, 29, 14, 704, 15, 900, 15, 1453, 15, 17, 275, 16, 15521, 15, 28, 134, 28, 1428, 1242, 26, 66, 569, 15, 19946, 13111, 15, 17, 676, 5511, 9, 158, 5740, 50, 147, 20, 2803, 6035, 30, 92, 6558, 26, 2840, 9, 15521, 92, 44, 147, 19, 186, 1858, 17, 26, 186, 2131, 15, 47, 786, 151, 50, 147, 19, 3342, 11246, 13, 5, 3970, 3816, 11643, 17, 121, 8, 19516, 857, 6, 15, 4587, 5102, 15, 54, 113, 2840, 1967, 4563, 9, 15521, 92, 247, 27, 186, 505, 47, 109, 50, 117, 20, 14381, 2840, 19, 1468, 9, 48, 25, 87, 20, 448, 19, 14, 10156, 16, 21, 8288, 19, 1200, 13426, 6142, 3257, 18, 951, 986, 34, 148, 9, 145, 15521, 1735, 20, 22156, 2094, 15, 8855, 15, 2974, 15, 26747, 15, 17, 11374, 602, 21, 585, 92, 107, 9, 3, 98, 107, 15521, 30, 14381, 2840, 944, 20, 107, 60, 3]\n",
            "The answer is: replicate walking, lifting, speech, cognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}