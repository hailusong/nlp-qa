{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-qa-albert-pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN7qEjIesNHWd0ebGTo98IZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_qa_albert_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "References:\n",
        "1. [AllenNLP Q&A with heatmap](https://demo.allennlp.org/reading-comprehension/MjE0MTE2MQ==)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "dbde61d8-558e-48f7-aa1f-f0120dbbace5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "!uname -a\n",
        "!apt-get install file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 948adb98e3a7 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 1s (236 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "outputId": "8ee87976-30ce-45c7-8cde-581e8ab37d78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.5MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 25.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=a72e720e2be4b6547cf591b9ee5c9be194d701d7a4010f3d49d0a05cbf945fdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "b27f117a-eb6e-4717-929c-df05bc0f0851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMf33lSSGpQ",
        "colab_type": "code",
        "outputId": "33bd948b-b1fd-40f6-96a2-fde38f11cb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!ls -al\n",
        "!rm -rf nlp-qa\n",
        "!git clone --depth 1 https://github.com/hailusong/nlp-qa.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 .\n",
            "drwxr-xr-x 1 root root 4096 Jun  4 18:40 ..\n",
            "drwxr-xr-x 1 root root 4096 Jun  2 16:14 .config\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 sample_data\n",
            "Cloning into 'nlp-qa'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "9b8ee1b1-5621-4e95-abce-fa637840ebca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -r nlp-qa/question-answering/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'nlp-qa/question-answering/requirements.txt'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MULTILABEL_INTENT=True\n",
        "QA_EXTRACTION=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained ALBERT v2 language model \n",
        "Ref:\n",
        "- https://huggingface.co/transformers/pretrained_models.html\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "outputId": "38ec142e-b038-4500-bc97-4b3202b5e6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "  from transformers import AlbertConfig, AlbertForSequenceClassification\n",
        "  import torch\n",
        "\n",
        "  config = torch.hub.load('huggingface/pytorch-transformers', 'config', 'albert-large-v2')\n",
        "  config.num_labels = 3\n",
        "  assert(type(config) == AlbertConfig)\n",
        "  print(config)\n",
        "\n",
        "  # Download model and configuration from S3 and cache.\n",
        "  # label: lawyer letter, deed, other\n",
        "  model = torch.hub.load('huggingface/pytorch-transformers', \n",
        "                         'modelForSequenceClassification', \n",
        "                         'albert-large-v2',\n",
        "                         config=config\n",
        "                         )\n",
        "  assert(type(model) == AlbertForSequenceClassification)\n",
        "\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'model', './test/bert_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'albert-large-v2', output_attentions=True)  # Update configuration during loading\n",
        "  # assert model.config.output_attentions == True\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\",\n",
            "    \"2\": \"LABEL_2\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1,\n",
            "    \"LABEL_2\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34KA-DEywd1",
        "colab_type": "code",
        "outputId": "cbfea1c6-3de8-4197-b66b-0496769f3beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  # Logging\n",
        "  print('\\n==== Output Layer ====\\n')\n",
        "  \n",
        "  params = list(model.named_parameters())\n",
        "  for p in params[-4:]:\n",
        "      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "albert.pooler.weight                                    (1024, 1024)\n",
            "albert.pooler.bias                                           (1024,)\n",
            "classifier.weight                                          (3, 1024)\n",
            "classifier.bias                                                 (3,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeQw1NmYogxY",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained ALBERT v2 model on SQuAD2\n",
        "https://huggingface.co/elgeish/cs224n-squad2.0-albert-xxlarge-v1, including:\n",
        "- config.json, 721.0B\n",
        "- pytorch_model.bin, 849.2MB\n",
        "- special_tokens_map.json, 156.0B\n",
        "- spiece.model, 742.5KB\n",
        "- tokenizer_config.json, 39.0B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYVCaev5YQEt",
        "colab_type": "code",
        "outputId": "e4998f6d-0afe-47fa-d25d-f75f88fec09f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  from transformers import ( AutoTokenizer, AutoModelForQuestionAnswering )\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(\"elgeish/cs224n-squad2.0-albert-xxlarge-v1\")\n",
        "  model = AutoModelForQuestionAnswering.from_pretrained(\"elgeish/cs224n-squad2.0-albert-xxlarge-v1\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils.py:831: FutureWarning: Parameter max_len is deprecated and will be removed in a future release. Use model_max_length instead.\n",
            "  category=FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmN-BLTg-Cp",
        "colab_type": "code",
        "outputId": "d2f5e869-22d7-4049-8693-2ee1ce184e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  # inspect tokenizer max length\n",
        "  print(f'Tokenizer max input length {tokenizer.model_max_length}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizer max input length 512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_0kkjUZ2eY4",
        "colab_type": "text"
      },
      "source": [
        "## Load Training Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMsq8Jq92hqY",
        "colab_type": "text"
      },
      "source": [
        "### Load ALBERT multi-label training data\n",
        "Some example datasets:\n",
        "-  The [Corpus of Linguistic Acceptability (CoLA)](https://nyu-mll.github.io/CoLA/) dataset for single sentence classification, which is about sentences labeled as grammatically correct or incorrect - part of GLUE benchmark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69BwAoop3B39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  import wget\n",
        "  import os\n",
        "\n",
        "  print('Downloading dataset...')\n",
        "\n",
        "  # The URL for the dataset zip file.\n",
        "  url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
        "\n",
        "  # Download the file (if we haven't already)\n",
        "  if not os.path.exists('./cola_public_1.1.zip'):\n",
        "      wget.download(url, './cola_public_1.1.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-6TYXW63EoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  # Unzip the dataset (if we haven't already)\n",
        "  if not os.path.exists('./cola_public/'):\n",
        "      !unzip cola_public_1.1.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQGgzX_z3H6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  # Get the lists of sentences and their labels.\n",
        "  sentences = df.sentence.values\n",
        "  labels = df.label.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTTULeIV28AC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  import pandas as pd\n",
        "\n",
        "  # Load the dataset into a pandas dataframe.\n",
        "  df = pd.read_csv(\"./cola_public/raw/in_domain_train.tsv\", delimiter='\\t', header=None, names=['sentence_source', 'label', 'label_notes', 'sentence'])\n",
        "\n",
        "  # Report the number of sentences.\n",
        "  print('Number of training sentences: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "  # Display 10 random rows from the data.\n",
        "  df.sample(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Itn0AJ3Y2uDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  max_len = 0\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
        "\n",
        "      # Update the maximum sentence length.\n",
        "      max_len = max(max_len, len(input_ids))\n",
        "\n",
        "  print('Max sentence length: ', max_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MFSxXoZ2ury",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for sent in sentences:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length`\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          sent,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = 64,           # Pad & truncate all sentences.\n",
        "                          pad_to_max_length = True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      input_ids.append(encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "  attention_masks = torch.cat(attention_masks, dim=0)\n",
        "  labels = torch.tensor(labels)\n",
        "\n",
        "  # Print sentence 0, now as a list of IDs.\n",
        "  print('Original: ', sentences[0])\n",
        "  print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZBLmWOx2ntJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "  # Combine the training inputs into a TensorDataset.\n",
        "  dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "  # Create a 90-10 train-validation split.\n",
        "\n",
        "  # Calculate the number of samples to include in each set.\n",
        "  train_size = int(0.9 * len(dataset))\n",
        "  val_size = len(dataset) - train_size\n",
        "\n",
        "  # Divide the dataset by randomly selecting samples.\n",
        "  train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "  print('{:>5,} training samples'.format(train_size))\n",
        "  print('{:>5,} validation samples'.format(val_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gtTbEP32gL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "  # The DataLoader needs to know our batch size for training, so we specify it \n",
        "  # here. For fine-tuning BERT on a specific task, the authors recommend a batch \n",
        "  # size of 16 or 32.\n",
        "  batch_size = 32\n",
        "\n",
        "  # Create the DataLoaders for our training and validation sets.\n",
        "  # We'll take training samples in random order. \n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  validation_dataloader = DataLoader(\n",
        "              val_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size # Evaluate with this batch size.\n",
        "          )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzzoWKjf1okN",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQDMF6PZ1rZr",
        "colab_type": "text"
      },
      "source": [
        "### Fine-tune the ALBERT multi-label classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUiyi6PM1v6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from transformers import AdamW\n",
        "\n",
        "  # Optimizer and LR setup\n",
        "  # Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "  # I believe the 'W' stands for 'Weight Decay fix\"\n",
        "  optimizer = AdamW(\n",
        "      model.parameters(),\n",
        "      lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-61o1eSE11SH",
        "colab_type": "code",
        "outputId": "d5a3b786-0de6-4c5d-cc5b-611139f05a31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "if MULTILABEL_INTENT:\n",
        "  from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "  # Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "  # We chose to run for 4, but we'll see later that this may be over-fitting the\n",
        "  # training data.\n",
        "  epochs = 4\n",
        "\n",
        "  # Total number of training steps is [number of batches] x [number of epochs]. \n",
        "  # (Note that this is not the same as the number of training samples).\n",
        "  total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "  # Create the learning rate scheduler.\n",
        "  scheduler = get_linear_schedule_with_warmup(\n",
        "      optimizer, \n",
        "      num_warmup_steps = 0, # Default value in run_glue.py\n",
        "      num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-88e92d925d65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# Total number of training steps is [number of batches] x [number of epochs].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# (Note that this is not the same as the number of training samples).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mtotal_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Create the learning rate scheduler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghMbXVpFKhr",
        "colab_type": "code",
        "outputId": "f114005f-72d9-43ba-eeb2-10cc41e7498f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'modelForQuestionAnswering', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "  # tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "  # The format is paragraph first and then question\n",
        "  # fact = \"Jim Henson was a puppeteer\"\n",
        "  # q = \"Who was Jim Henson ?\"\n",
        "  fact = input('Fact: ')\n",
        "  q = input('Question: ')\n",
        "\n",
        "  # Converts a list of string to a sequence of ids (integer), using the tokenizer and vocabulary. \n",
        "  # Also request to add model-specific special tokens (such as beginning of sequence, end of sequence, sequence separator).\n",
        "  indexed_tokens_fact = tokenizer.encode(fact, add_special_tokens=True)\n",
        "  indexed_tokens_q = tokenizer.encode(q, add_special_tokens=True)\n",
        "\n",
        "  # merge fact tokens and Q tokens by removing the START token in Q tokens\n",
        "  indexed_tokens = indexed_tokens_fact + indexed_tokens_q[1:]\n",
        "  segments_ids = [0] * len(indexed_tokens_fact) + [1] * len(indexed_tokens_q[1:])\n",
        "\n",
        "  print(f'Length is {len(indexed_tokens)}, Indexed token: {indexed_tokens}')\n",
        "\n",
        "  # Predict the start and end positions logits\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      start_logits, end_logits = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "  # get the highest prediction\n",
        "  answer = tokenizer.decode(indexed_tokens[torch.argmax(start_logits):torch.argmax(end_logits)+1])\n",
        "  print(f'The answer is: {answer}')\n",
        "\n",
        "  # Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n",
        "  # start_positions, end_positions = torch.tensor([12]), torch.tensor([14])\n",
        "  # multiple_choice_loss = model(tokens_tensor, token_type_ids=segments_tensors, start_positions=start_positions, end_positions=end_positions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fact: Robotics is an interdisciplinary branch of engineering and science that includes mechanical engineering, electrical engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing. These technologies are used to develop machines that can substitute for humans. Robots can be used in any situation and for any purpose, but today many are used in dangerous environments (including bomb detection and de-activation), manufacturing processes, or where humans cannot survive. Robots can take on any form but some are made to resemble humans in appearance. This is said to help in the acceptance of a robot in certain replicative behaviors usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, and basically anything a human can do.\n",
            "Question: What do robots that resemble humans plan  to do?\n",
            "Length is 183, Indexed token: [2, 15219, 18, 25, 40, 19, 20745, 1686, 16, 1552, 17, 762, 30, 1103, 4953, 1552, 15, 4618, 1552, 15, 1428, 762, 15, 17, 654, 9, 15219, 18, 10342, 29, 14, 704, 15, 900, 15, 1453, 15, 17, 275, 16, 15521, 15, 28, 134, 28, 1428, 1242, 26, 66, 569, 15, 19946, 13111, 15, 17, 676, 5511, 9, 158, 5740, 50, 147, 20, 2803, 6035, 30, 92, 6558, 26, 2840, 9, 15521, 92, 44, 147, 19, 186, 1858, 17, 26, 186, 2131, 15, 47, 786, 151, 50, 147, 19, 3342, 11246, 13, 5, 3970, 3816, 11643, 17, 121, 8, 19516, 857, 6, 15, 4587, 5102, 15, 54, 113, 2840, 1967, 4563, 9, 15521, 92, 247, 27, 186, 505, 47, 109, 50, 117, 20, 14381, 2840, 19, 1468, 9, 48, 25, 87, 20, 448, 19, 14, 10156, 16, 21, 8288, 19, 1200, 13426, 6142, 3257, 18, 951, 986, 34, 148, 9, 145, 15521, 1735, 20, 22156, 2094, 15, 8855, 15, 2974, 15, 26747, 15, 17, 11374, 602, 21, 585, 92, 107, 9, 3, 98, 107, 15521, 30, 14381, 2840, 944, 20, 107, 60, 3]\n",
            "The answer is: replicate walking, lifting, speech, cognition\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}