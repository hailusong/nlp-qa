{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-qa-diy-finetune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOp4Xn6FZ0dTo3J819lUEyj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_qa_diy_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "abd171ed-78d2-4ad9-ab91-c2f9f95df81a"
      },
      "source": [
        "!uname -a\n",
        "!apt-get install file"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 21e96c7b6fa1 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libmagic-mgc libmagic1\n",
            "The following NEW packages will be installed:\n",
            "  file libmagic-mgc libmagic1\n",
            "0 upgraded, 3 newly installed, 0 to remove and 32 not upgraded.\n",
            "Need to get 275 kB of archives.\n",
            "After this operation, 5,297 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 file amd64 1:5.32-2ubuntu0.4 [22.1 kB]\n",
            "Fetched 275 kB in 1s (377 kB/s)\n",
            "Selecting previously unselected package libmagic-mgc.\n",
            "(Reading database ... 144467 files and directories currently installed.)\n",
            "Preparing to unpack .../libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package libmagic1:amd64.\n",
            "Preparing to unpack .../libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Selecting previously unselected package file.\n",
            "Preparing to unpack .../file_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking file (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
            "Setting up file (1:5.32-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "6de33347-fe34-4dde-a9e8-291af1c1118c"
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674kB)\n",
            "\u001b[K     |████████████████████████████████| 675kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 16.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=61bb22395a7908723d542a1c30c042c6ec3fae10f8fff43cb0c05ae3a289f469\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "518e6956-7c32-4f77-9868-0e3a47428480"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMf33lSSGpQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3eddfc0a-d57a-48f4-a35d-a2bc03a24015"
      },
      "source": [
        "!ls -al\n",
        "!rm -rf nlp-qa\n",
        "!git clone --depth 1 https://github.com/hailusong/nlp-qa.git"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 .\n",
            "drwxr-xr-x 1 root root 4096 Jun  3 19:39 ..\n",
            "drwxr-xr-x 1 root root 4096 Jun  2 16:14 .config\n",
            "drwxr-xr-x 1 root root 4096 May 29 18:19 sample_data\n",
            "Cloning into 'nlp-qa'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1de76dbc-a6d3-4860-8a89-72e711f5104c"
      },
      "source": [
        "!pip install -r nlp-qa/question-answering/requirements.txt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r nlp-qa/question-answering/requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r nlp-qa/question-answering/requirements.txt (line 2)) (0.22.2.post1)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from -r nlp-qa/question-answering/requirements.txt (line 4)) (5.4.8)\n",
            "Collecting sacrebleu\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/4b/6c7a0b26a48d88f56573d11aa5058808fe0d36ba40951287894f943556b5/sacrebleu-1.4.10-py3-none-any.whl (60kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/6d/2b9a64cba1e4e6ecd4effbf6834b2592b54dc813654f84029758e5daeeb5/rouge_score-0.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.6/dist-packages (from -r nlp-qa/question-answering/requirements.txt (line 7)) (2.1.0)\n",
            "Collecting pytorch-lightning==0.7.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/53/0549dd9c44c90e96d217592e094e9c53ef39ae2fed0c5cdb7e57aca65af6/pytorch_lightning-0.7.3-py3-none-any.whl (203kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r nlp-qa/question-answering/requirements.txt (line 9)) (3.2.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.29.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.6.0.post3)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.18.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (47.1.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (0.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.7.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r nlp-qa/question-answering/requirements.txt (line 2)) (0.15.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r nlp-qa/question-answering/requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval->-r nlp-qa/question-answering/requirements.txt (line 3)) (2.3.1)\n",
            "Collecting portalocker\n",
            "  Downloading https://files.pythonhosted.org/packages/53/84/7b3146ec6378d28abc73ab484f09f47dfa008ad6f03f33d90a369f880e25/portalocker-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from rouge-score->-r nlp-qa/question-answering/requirements.txt (line 6)) (3.2.5)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (19.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (0.3.1.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (0.22.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (4.41.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (2.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (1.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (1.12.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.7.3->-r nlp-qa/question-answering/requirements.txt (line 8)) (1.5.0+cu101)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r nlp-qa/question-answering/requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r nlp-qa/question-answering/requirements.txt (line 9)) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r nlp-qa/question-answering/requirements.txt (line 9)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r nlp-qa/question-answering/requirements.txt (line 9)) (0.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (2020.4.5.1)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r nlp-qa/question-answering/requirements.txt (line 3)) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r nlp-qa/question-answering/requirements.txt (line 3)) (2.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r nlp-qa/question-answering/requirements.txt (line 3)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval->-r nlp-qa/question-answering/requirements.txt (line 3)) (3.13)\n",
            "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow_datasets->-r nlp-qa/question-answering/requirements.txt (line 7)) (1.51.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r nlp-qa/question-answering/requirements.txt (line 1)) (3.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=67e45a4174d8b02069294cbbf65e51781aa6ae51528b9e0b6be41c725e86dac7\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "\u001b[31mERROR: pytorch-lightning 0.7.3 has requirement future>=0.17.1, but you'll have future 0.16.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: seqeval, portalocker, sacrebleu, rouge-score, pytorch-lightning\n",
            "Successfully installed portalocker-1.7.0 pytorch-lightning-0.7.3 rouge-score-0.0.3 sacrebleu-1.4.10 seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tune on SQuAD 1.1 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SgOfVmll18v",
        "colab_type": "text"
      },
      "source": [
        "### Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOjePghol02j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "fd778319-a653-4fd2-ce8b-c522606023c5"
      },
      "source": [
        "%env SQUAD_DIR=./SQUAD-10-dataset\n",
        "!rmdir -f $SQUAD_DIR\n",
        "!mkdir -p $SQUAD_DIR\n",
        "!wget -O $SQUAD_DIR/train-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
        "!wget -O $SQUAD_DIR/dev-v1.1.json https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
        "!wget -O $SQUAD_DIR/evaluate-v1.1.py https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: SQUAD_DIR=./SQUAD-10-dataset\n",
            "rmdir: invalid option -- 'f'\n",
            "Try 'rmdir --help' for more information.\n",
            "--2020-06-03 19:45:07--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30288272 (29M) [application/json]\n",
            "Saving to: ‘./SQUAD-10-dataset/train-v1.1.json’\n",
            "\n",
            "./SQUAD-10-dataset/ 100%[===================>]  28.88M  46.2MB/s    in 0.6s    \n",
            "\n",
            "2020-06-03 19:45:08 (46.2 MB/s) - ‘./SQUAD-10-dataset/train-v1.1.json’ saved [30288272/30288272]\n",
            "\n",
            "--2020-06-03 19:45:09--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.111.153, 185.199.109.153, 185.199.108.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.111.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4854279 (4.6M) [application/json]\n",
            "Saving to: ‘./SQUAD-10-dataset/dev-v1.1.json’\n",
            "\n",
            "./SQUAD-10-dataset/ 100%[===================>]   4.63M  25.7MB/s    in 0.2s    \n",
            "\n",
            "2020-06-03 19:45:10 (25.7 MB/s) - ‘./SQUAD-10-dataset/dev-v1.1.json’ saved [4854279/4854279]\n",
            "\n",
            "--2020-06-03 19:45:10--  https://github.com/allenai/bi-att-flow/blob/master/squad/evaluate-v1.1.py\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/html]\n",
            "Saving to: ‘./SQUAD-10-dataset/evaluate-v1.1.py’\n",
            "\n",
            "./SQUAD-10-dataset/     [ <=>                ] 113.13K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-06-03 19:45:11 (1.45 MB/s) - ‘./SQUAD-10-dataset/evaluate-v1.1.py’ saved [115850]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "### Fine-tune\n",
        "run_squad.py should download following data from net:\n",
        "- model parameter configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json\n",
        "- vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt\n",
        "- weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkWv-JiJFHDZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "24d3cf91-d972-4f3f-8227-cf018f225ad9"
      },
      "source": [
        "!file -i $SQUAD_DIR/train-v1.1.json\n",
        "# !iconv -f utf-8 -t ascii//TRANSLIT < $SQUAD_DIR/train-v1.1.json > $SQUAD_DIR/train-v1.1-ascii.json \n",
        "!head -c 50 $SQUAD_DIR/train-v1.1.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./SQUAD-10-dataset/train-v1.1.json: text/plain; charset=us-ascii\n",
            "{\"data\": [{\"title\": \"University_of_Notre_Dame\", \"p"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBbXNs78Db0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c16e2df8-569b-456d-8e68-7fa7c4a11862"
      },
      "source": [
        "!python ./nlp-qa/question-answering/run_squad.py \\\n",
        "  --model_type bert \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_lower_case \\\n",
        "  --train_file $SQUAD_DIR/train-v1.1.json \\\n",
        "  --predict_file $SQUAD_DIR/dev-v1.1.json \\\n",
        "  --per_gpu_train_batch_size 12 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 2.0 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir /tmp/debug_squad/"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-03 19:45:15.963225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "06/03/2020 19:45:18 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "06/03/2020 19:45:18 - INFO - filelock -   Lock 140284826297960 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "06/03/2020 19:45:18 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpn3bw7oit\n",
            "Downloading: 100% 433/433 [00:00<00:00, 324kB/s]\n",
            "06/03/2020 19:45:18 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "06/03/2020 19:45:18 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "06/03/2020 19:45:18 - INFO - filelock -   Lock 140284826297960 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "06/03/2020 19:45:18 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "06/03/2020 19:45:18 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "06/03/2020 19:45:18 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "06/03/2020 19:45:18 - INFO - transformers.configuration_utils -   Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "06/03/2020 19:45:18 - INFO - filelock -   Lock 140284826270744 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "06/03/2020 19:45:18 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpvailqv_v\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.80MB/s]\n",
            "06/03/2020 19:45:19 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/03/2020 19:45:19 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/03/2020 19:45:19 - INFO - filelock -   Lock 140284826270744 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "06/03/2020 19:45:19 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "06/03/2020 19:45:19 - INFO - filelock -   Lock 140284826269400 acquired on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "06/03/2020 19:45:19 - INFO - transformers.file_utils -   https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpnf1q74fz\n",
            "Downloading: 100% 440M/440M [00:08<00:00, 52.3MB/s]\n",
            "06/03/2020 19:45:27 - INFO - transformers.file_utils -   storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "06/03/2020 19:45:27 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "06/03/2020 19:45:27 - INFO - filelock -   Lock 140284826269400 released on /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157.lock\n",
            "06/03/2020 19:45:27 - INFO - transformers.modeling_utils -   loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "06/03/2020 19:45:31 - INFO - transformers.modeling_utils -   Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "06/03/2020 19:45:31 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "06/03/2020 19:45:31 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir=None, device=device(type='cpu'), do_eval=True, do_lower_case=True, do_train=True, doc_stride=128, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, lang_id=0, learning_rate=3e-05, local_rank=-1, logging_steps=500, max_answer_length=30, max_grad_norm=1.0, max_query_length=64, max_seq_length=384, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_best_size=20, n_gpu=0, no_cuda=False, null_score_diff_threshold=0.0, num_train_epochs=2.0, output_dir='/tmp/debug_squad/', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=8, per_gpu_train_batch_size=12, predict_file='./SQUAD-10-dataset/dev-v1.1.json', save_steps=500, seed=42, server_ip='', server_port='', threads=1, tokenizer_name='', train_file='./SQUAD-10-dataset/train-v1.1.json', verbose_logging=False, version_2_with_negative=False, warmup_steps=0, weight_decay=0.0)\n",
            "06/03/2020 19:45:31 - INFO - __main__ -   Creating features from dataset file at .\n",
            "100% 442/442 [00:32<00:00, 13.59it/s]\n",
            "convert squad examples to features: 100% 87599/87599 [14:26<00:00, 101.08it/s]\n",
            "add example index and unique id: 100% 87599/87599 [00:00<00:00, 822911.54it/s]\n",
            "06/03/2020 20:00:34 - INFO - __main__ -   Saving features into cached file ./cached_train_bert-base-uncased_384\n",
            "tcmalloc: large alloc 1178583040 bytes == 0x1b5876000 @  0x7f97445a92a4 0x591d67 0x4dd6a7 0x4dd73f 0x4e1d6d 0x4e22d3 0x4e293c 0x4e0d98 0x4e251b 0x4e22fb 0x4e2990 0x4e3386 0x5eb3d2 0x50a35c 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x509a90 0x50a48d 0x50cd96 0x507d64 0x509a90 0x50a48d 0x50bfb4\n",
            "06/03/2020 20:02:57 - INFO - __main__ -   ***** Running training *****\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Num examples = 88641\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Num Epochs = 2\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Instantaneous batch size per GPU = 12\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 12\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "06/03/2020 20:02:57 - INFO - __main__ -     Total optimization steps = 14774\n",
            "Epoch:   0% 0/2 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/7387 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/7387 [00:52<107:10:24, 52.24s/it]\u001b[A\n",
            "Iteration:   0% 2/7387 [01:43<106:29:00, 51.91s/it]\u001b[A\n",
            "Iteration:   0% 3/7387 [02:34<105:41:46, 51.53s/it]\u001b[A\n",
            "Iteration:   0% 4/7387 [03:24<105:12:24, 51.30s/it]\u001b[A\n",
            "Iteration:   0% 5/7387 [04:15<104:37:58, 51.03s/it]\u001b[A\n",
            "Iteration:   0% 6/7387 [05:05<104:12:18, 50.82s/it]\u001b[A\n",
            "Iteration:   0% 7/7387 [05:56<104:00:57, 50.74s/it]\u001b[A\n",
            "Iteration:   0% 8/7387 [06:46<104:05:08, 50.78s/it]\u001b[A\n",
            "Iteration:   0% 9/7387 [07:38<104:16:41, 50.88s/it]\u001b[A\n",
            "Iteration:   0% 10/7387 [08:28<104:02:16, 50.77s/it]\u001b[A\n",
            "Iteration:   0% 11/7387 [09:19<103:55:08, 50.72s/it]\u001b[A\n",
            "Iteration:   0% 12/7387 [10:09<103:56:22, 50.74s/it]\u001b[A\n",
            "Iteration:   0% 13/7387 [11:00<103:55:18, 50.73s/it]\u001b[A\n",
            "Iteration:   0% 14/7387 [11:51<103:53:18, 50.73s/it]\u001b[A\n",
            "Iteration:   0% 15/7387 [12:43<104:59:41, 51.27s/it]\u001b[A\n",
            "Iteration:   0% 16/7387 [13:34<104:33:38, 51.07s/it]\u001b[A\n",
            "Iteration:   0% 17/7387 [14:25<104:11:41, 50.90s/it]\u001b[A\n",
            "Iteration:   0% 18/7387 [15:15<104:01:16, 50.82s/it]\u001b[A\n",
            "Iteration:   0% 19/7387 [16:06<103:57:51, 50.80s/it]\u001b[A\n",
            "Iteration:   0% 20/7387 [16:57<104:05:31, 50.87s/it]\u001b[A\n",
            "Iteration:   0% 21/7387 [17:49<104:49:14, 51.23s/it]\u001b[A\n",
            "Iteration:   0% 22/7387 [18:42<105:36:34, 51.62s/it]\u001b[A\n",
            "Iteration:   0% 23/7387 [19:33<105:11:56, 51.43s/it]\u001b[A\n",
            "Iteration:   0% 24/7387 [20:24<105:01:58, 51.35s/it]\u001b[A\n",
            "Iteration:   0% 25/7387 [21:15<105:01:12, 51.35s/it]\u001b[A\n",
            "Iteration:   0% 26/7387 [22:06<104:47:27, 51.25s/it]\u001b[A\n",
            "Iteration:   0% 27/7387 [22:58<105:07:42, 51.42s/it]\u001b[A\n",
            "Iteration:   0% 28/7387 [23:50<105:21:14, 51.54s/it]\u001b[A\n",
            "Iteration:   0% 29/7387 [24:41<105:00:43, 51.38s/it]\u001b[A\n",
            "Iteration:   0% 30/7387 [25:32<104:42:20, 51.24s/it]\u001b[A\n",
            "Iteration:   0% 31/7387 [26:23<104:39:49, 51.22s/it]\u001b[A\n",
            "Iteration:   0% 32/7387 [27:14<104:41:48, 51.25s/it]\u001b[A\n",
            "Iteration:   0% 33/7387 [28:06<105:09:14, 51.48s/it]\u001b[A\n",
            "Iteration:   0% 34/7387 [28:58<105:23:02, 51.60s/it]\u001b[A\n",
            "Iteration:   0% 35/7387 [29:49<104:48:51, 51.32s/it]\u001b[A\n",
            "Iteration:   0% 36/7387 [30:39<104:25:44, 51.14s/it]\u001b[A\n",
            "Iteration:   1% 37/7387 [31:30<104:04:14, 50.97s/it]\u001b[A\n",
            "Iteration:   1% 38/7387 [32:21<103:56:51, 50.92s/it]\u001b[A\n",
            "Iteration:   1% 39/7387 [33:12<103:55:14, 50.91s/it]\u001b[A\n",
            "Iteration:   1% 40/7387 [34:03<104:04:55, 51.00s/it]\u001b[A\n",
            "Iteration:   1% 41/7387 [34:54<103:57:20, 50.94s/it]\u001b[A\n",
            "Iteration:   1% 42/7387 [35:44<103:38:09, 50.79s/it]\u001b[A\n",
            "Iteration:   1% 43/7387 [36:35<103:31:07, 50.74s/it]\u001b[A\n",
            "Iteration:   1% 44/7387 [37:25<103:29:11, 50.74s/it]\u001b[A\n",
            "Iteration:   1% 45/7387 [38:17<103:44:20, 50.87s/it]\u001b[A\n",
            "Iteration:   1% 46/7387 [39:07<103:28:23, 50.74s/it]\u001b[A\n",
            "Iteration:   1% 47/7387 [39:58<103:50:02, 50.93s/it]\u001b[A\n",
            "Iteration:   1% 48/7387 [40:49<103:33:53, 50.80s/it]\u001b[A\n",
            "Iteration:   1% 49/7387 [41:39<103:20:13, 50.70s/it]\u001b[A\n",
            "Iteration:   1% 50/7387 [42:30<103:18:31, 50.69s/it]\u001b[A\n",
            "Iteration:   1% 51/7387 [43:21<103:15:56, 50.68s/it]\u001b[A\n",
            "Iteration:   1% 52/7387 [44:11<103:06:15, 50.60s/it]\u001b[A\n",
            "Iteration:   1% 53/7387 [45:02<103:08:55, 50.63s/it]\u001b[A\n",
            "Iteration:   1% 54/7387 [45:52<103:01:34, 50.58s/it]\u001b[A\n",
            "Iteration:   1% 55/7387 [46:43<102:58:40, 50.56s/it]\u001b[A\n",
            "Iteration:   1% 56/7387 [47:33<102:55:29, 50.54s/it]\u001b[A\n",
            "Iteration:   1% 57/7387 [48:24<102:54:49, 50.54s/it]\u001b[A\n",
            "Iteration:   1% 58/7387 [49:14<102:52:46, 50.53s/it]\u001b[A\n",
            "Iteration:   1% 59/7387 [50:05<102:49:38, 50.52s/it]\u001b[A\n",
            "Iteration:   1% 60/7387 [50:56<103:14:05, 50.72s/it]\u001b[A\n",
            "Iteration:   1% 61/7387 [51:47<103:08:09, 50.68s/it]\u001b[A\n",
            "Iteration:   1% 62/7387 [52:37<103:08:01, 50.69s/it]\u001b[A\n",
            "Iteration:   1% 63/7387 [53:28<102:58:02, 50.61s/it]\u001b[A\n",
            "Iteration:   1% 64/7387 [54:18<102:53:51, 50.58s/it]\u001b[A\n",
            "Iteration:   1% 65/7387 [55:09<102:43:04, 50.50s/it]\u001b[A\n",
            "Iteration:   1% 66/7387 [55:59<102:44:17, 50.52s/it]\u001b[A\n",
            "Iteration:   1% 67/7387 [56:50<102:44:27, 50.53s/it]\u001b[A\n",
            "Iteration:   1% 68/7387 [57:40<102:47:34, 50.56s/it]\u001b[A\n",
            "Iteration:   1% 69/7387 [58:31<102:51:26, 50.60s/it]\u001b[A\n",
            "Iteration:   1% 70/7387 [59:22<102:49:59, 50.59s/it]\u001b[A\n",
            "Iteration:   1% 71/7387 [1:00:12<102:42:29, 50.54s/it]\u001b[A\n",
            "Iteration:   1% 72/7387 [1:01:02<102:36:39, 50.50s/it]\u001b[A\n",
            "Iteration:   1% 73/7387 [1:01:53<102:29:01, 50.44s/it]\u001b[A\n",
            "Iteration:   1% 74/7387 [1:02:43<102:30:46, 50.46s/it]\u001b[A\n",
            "Iteration:   1% 75/7387 [1:03:34<102:24:58, 50.42s/it]\u001b[A\n",
            "Iteration:   1% 76/7387 [1:04:24<102:36:31, 50.53s/it]\u001b[A\n",
            "Iteration:   1% 77/7387 [1:05:15<102:31:29, 50.49s/it]\u001b[A\n",
            "Iteration:   1% 78/7387 [1:06:05<102:32:54, 50.51s/it]\u001b[A\n",
            "Iteration:   1% 79/7387 [1:06:56<102:37:03, 50.55s/it]\u001b[A\n",
            "Iteration:   1% 80/7387 [1:07:47<102:38:04, 50.57s/it]\u001b[A\n",
            "Iteration:   1% 81/7387 [1:08:37<102:33:35, 50.54s/it]\u001b[A\n",
            "Iteration:   1% 82/7387 [1:09:28<102:34:28, 50.55s/it]\u001b[A\n",
            "Iteration:   1% 83/7387 [1:10:18<102:32:29, 50.54s/it]\u001b[A\n",
            "Iteration:   1% 84/7387 [1:11:09<102:26:30, 50.50s/it]\u001b[A\n",
            "Iteration:   1% 85/7387 [1:11:59<102:23:40, 50.48s/it]\u001b[A\n",
            "Iteration:   1% 86/7387 [1:12:50<102:36:09, 50.59s/it]\u001b[A\n",
            "Iteration:   1% 87/7387 [1:13:40<102:32:33, 50.57s/it]\u001b[A\n",
            "Iteration:   1% 88/7387 [1:14:31<102:35:50, 50.60s/it]\u001b[A\n",
            "Iteration:   1% 89/7387 [1:15:22<102:36:04, 50.61s/it]\u001b[A\n",
            "Iteration:   1% 90/7387 [1:16:13<103:02:58, 50.84s/it]\u001b[A\n",
            "Iteration:   1% 91/7387 [1:17:04<103:21:30, 51.00s/it]\u001b[A\n",
            "Iteration:   1% 92/7387 [1:17:56<103:32:18, 51.09s/it]\u001b[A\n",
            "Iteration:   1% 93/7387 [1:18:47<103:41:14, 51.18s/it]\u001b[A\n",
            "Iteration:   1% 94/7387 [1:19:39<103:49:06, 51.25s/it]\u001b[A\n",
            "Iteration:   1% 95/7387 [1:20:29<103:36:20, 51.15s/it]\u001b[A\n",
            "Iteration:   1% 96/7387 [1:21:20<103:19:07, 51.01s/it]\u001b[A\n",
            "Iteration:   1% 97/7387 [1:22:11<103:12:30, 50.97s/it]\u001b[A\n",
            "Iteration:   1% 98/7387 [1:23:02<103:19:59, 51.04s/it]\u001b[A\n",
            "Iteration:   1% 99/7387 [1:23:53<103:02:09, 50.90s/it]\u001b[A\n",
            "Iteration:   1% 100/7387 [1:24:44<103:08:24, 50.95s/it]\u001b[A\n",
            "Iteration:   1% 101/7387 [1:25:35<103:04:44, 50.93s/it]\u001b[A\n",
            "Iteration:   1% 102/7387 [1:26:26<103:02:27, 50.92s/it]\u001b[A\n",
            "Iteration:   1% 103/7387 [1:27:17<103:17:54, 51.05s/it]\u001b[A\n",
            "Iteration:   1% 104/7387 [1:28:09<103:38:16, 51.23s/it]\u001b[A\n",
            "Iteration:   1% 105/7387 [1:28:59<103:22:36, 51.11s/it]\u001b[A\n",
            "Iteration:   1% 106/7387 [1:29:52<104:10:43, 51.51s/it]\u001b[A\n",
            "Iteration:   1% 107/7387 [1:30:43<104:11:35, 51.52s/it]\u001b[A\n",
            "Iteration:   1% 108/7387 [1:31:35<103:59:46, 51.43s/it]\u001b[A\n",
            "Iteration:   1% 109/7387 [1:32:26<103:53:54, 51.39s/it]\u001b[A\n",
            "Iteration:   1% 110/7387 [1:33:17<103:52:09, 51.39s/it]\u001b[A\n",
            "Iteration:   2% 111/7387 [1:34:09<104:07:17, 51.52s/it]\u001b[A\n",
            "Iteration:   2% 112/7387 [1:35:01<104:01:58, 51.48s/it]\u001b[A\n",
            "Iteration:   2% 113/7387 [1:35:53<104:20:50, 51.64s/it]\u001b[A\n",
            "Iteration:   2% 114/7387 [1:36:44<104:03:43, 51.51s/it]\u001b[A\n",
            "Iteration:   2% 115/7387 [1:37:35<103:47:53, 51.39s/it]\u001b[A\n",
            "Iteration:   2% 116/7387 [1:38:26<103:33:15, 51.27s/it]\u001b[A\n",
            "Iteration:   2% 117/7387 [1:39:17<103:12:12, 51.10s/it]\u001b[A\n",
            "Iteration:   2% 118/7387 [1:40:07<103:02:26, 51.03s/it]\u001b[A\n",
            "Iteration:   2% 119/7387 [1:40:58<102:51:03, 50.94s/it]\u001b[A\n",
            "Iteration:   2% 120/7387 [1:41:49<102:54:09, 50.98s/it]\u001b[A\n",
            "Iteration:   2% 121/7387 [1:42:41<103:26:06, 51.25s/it]\u001b[A\n",
            "Iteration:   2% 122/7387 [1:43:32<103:28:59, 51.28s/it]\u001b[A\n",
            "Iteration:   2% 123/7387 [1:44:24<103:33:45, 51.33s/it]\u001b[A\n",
            "Iteration:   2% 124/7387 [1:45:15<103:32:55, 51.33s/it]\u001b[A\n",
            "Iteration:   2% 125/7387 [1:46:07<103:37:19, 51.37s/it]\u001b[A\n",
            "Iteration:   2% 126/7387 [1:46:58<103:20:53, 51.24s/it]\u001b[A\n",
            "Iteration:   2% 127/7387 [1:47:48<102:58:03, 51.06s/it]\u001b[A\n",
            "Iteration:   2% 128/7387 [1:48:39<102:53:34, 51.03s/it]\u001b[A\n",
            "Iteration:   2% 129/7387 [1:49:30<102:37:22, 50.90s/it]\u001b[A\n",
            "Iteration:   2% 130/7387 [1:50:21<102:44:01, 50.96s/it]\u001b[A\n",
            "Iteration:   2% 131/7387 [1:51:12<102:38:58, 50.93s/it]\u001b[A\n",
            "Iteration:   2% 132/7387 [1:52:03<102:30:10, 50.86s/it]\u001b[A\n",
            "Iteration:   2% 133/7387 [1:52:54<102:36:56, 50.93s/it]\u001b[A\n",
            "Iteration:   2% 134/7387 [1:53:45<102:44:46, 51.00s/it]\u001b[A\n",
            "Iteration:   2% 135/7387 [1:54:35<102:34:20, 50.92s/it]\u001b[A\n",
            "Iteration:   2% 136/7387 [1:55:26<102:34:20, 50.93s/it]\u001b[A\n",
            "Iteration:   2% 137/7387 [1:56:18<102:59:14, 51.14s/it]\u001b[A\n",
            "Iteration:   2% 138/7387 [1:57:09<102:47:40, 51.05s/it]\u001b[A\n",
            "Iteration:   2% 139/7387 [1:58:00<103:04:36, 51.20s/it]\u001b[A\n",
            "Iteration:   2% 140/7387 [1:58:52<103:10:42, 51.25s/it]\u001b[A\n",
            "Iteration:   2% 141/7387 [1:59:43<102:50:03, 51.09s/it]\u001b[A\n",
            "Iteration:   2% 142/7387 [2:00:34<103:00:04, 51.18s/it]\u001b[A\n",
            "Iteration:   2% 143/7387 [2:01:25<102:51:32, 51.12s/it]\u001b[A\n",
            "Iteration:   2% 144/7387 [2:02:16<102:44:50, 51.07s/it]\u001b[A^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghMbXVpFKhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}