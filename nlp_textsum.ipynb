{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-textsum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOUD5K/hzLYlUH73MEKNSIy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_textsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "#### **Text Summarization**\n",
        "Hyper-params:\n",
        "TBA\n",
        "\n",
        "Issues:\n",
        "1. Max sentence length: 64 (truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of Huggingface transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "debbb26b-2826-472e-f9d5-c7559c1c70e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!uname -a\n",
        "!pip install wget\n",
        "!apt-get install file"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 540c14d89df2 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "file is already the newest version (1:5.32-2ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "3e1f4ff6-a97b-4e04-f529-07fab8cf3dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLE_MODE=True\n",
        "INLINE_MODE=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "2bfadaeb-0d19-47bc-9e79-d7d85f00bfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  # ! [ ! -d \"./transformers\" ] && git clone --depth 1 https://github.com/huggingface/transformers\n",
        "  # ! [ -d \"./transformers\" ] && \\\n",
        "  #   cd transformers && \\\n",
        "  #   git fetch -a --tags && \\\n",
        "  #   git checkout tags/v2.11.0 && \\\n",
        "  #   pip install . && \\\n",
        "  ![ ! -d \"./nlp-qa\" ] && git clone --depth 1 https://github.com/hailusong/nlp-qa.git\n",
        "  !pip install nltk py-rouge"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-qa'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/36)\u001b[K\rremote: Counting objects:   5% (2/36)\u001b[K\rremote: Counting objects:   8% (3/36)\u001b[K\rremote: Counting objects:  11% (4/36)\u001b[K\rremote: Counting objects:  13% (5/36)\u001b[K\rremote: Counting objects:  16% (6/36)\u001b[K\rremote: Counting objects:  19% (7/36)\u001b[K\rremote: Counting objects:  22% (8/36)\u001b[K\rremote: Counting objects:  25% (9/36)\u001b[K\rremote: Counting objects:  27% (10/36)\u001b[K\rremote: Counting objects:  30% (11/36)\u001b[K\rremote: Counting objects:  33% (12/36)\u001b[K\rremote: Counting objects:  36% (13/36)\u001b[K\rremote: Counting objects:  38% (14/36)\u001b[K\rremote: Counting objects:  41% (15/36)\u001b[K\rremote: Counting objects:  44% (16/36)\u001b[K\rremote: Counting objects:  47% (17/36)\u001b[K\rremote: Counting objects:  50% (18/36)\u001b[K\rremote: Counting objects:  52% (19/36)\u001b[K\rremote: Counting objects:  55% (20/36)\u001b[K\rremote: Counting objects:  58% (21/36)\u001b[K\rremote: Counting objects:  61% (22/36)\u001b[K\rremote: Counting objects:  63% (23/36)\u001b[K\rremote: Counting objects:  66% (24/36)\u001b[K\rremote: Counting objects:  69% (25/36)\u001b[K\rremote: Counting objects:  72% (26/36)\u001b[K\rremote: Counting objects:  75% (27/36)\u001b[K\rremote: Counting objects:  77% (28/36)\u001b[K\rremote: Counting objects:  80% (29/36)\u001b[K\rremote: Counting objects:  83% (30/36)\u001b[K\rremote: Counting objects:  86% (31/36)\u001b[K\rremote: Counting objects:  88% (32/36)\u001b[K\rremote: Counting objects:  91% (33/36)\u001b[K\rremote: Counting objects:  94% (34/36)\u001b[K\rremote: Counting objects:  97% (35/36)\u001b[K\rremote: Counting objects: 100% (36/36)\u001b[K\rremote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects:   3% (1/31)\u001b[K\rremote: Compressing objects:   6% (2/31)\u001b[K\rremote: Compressing objects:   9% (3/31)\u001b[K\rremote: Compressing objects:  12% (4/31)\u001b[K\rremote: Compressing objects:  16% (5/31)\u001b[K\rremote: Compressing objects:  19% (6/31)\u001b[K\rremote: Compressing objects:  22% (7/31)\u001b[K\rremote: Compressing objects:  25% (8/31)\u001b[K\rremote: Compressing objects:  29% (9/31)\u001b[K\rremote: Compressing objects:  32% (10/31)\u001b[K\rremote: Compressing objects:  35% (11/31)\u001b[K\rremote: Compressing objects:  38% (12/31)\u001b[K\rremote: Compressing objects:  41% (13/31)\u001b[K\rremote: Compressing objects:  45% (14/31)\u001b[K\rremote: Compressing objects:  48% (15/31)\u001b[K\rremote: Compressing objects:  51% (16/31)\u001b[K\rremote: Compressing objects:  54% (17/31)\u001b[K\rremote: Compressing objects:  58% (18/31)\u001b[K\rremote: Compressing objects:  61% (19/31)\u001b[K\rremote: Compressing objects:  64% (20/31)\u001b[K\rremote: Compressing objects:  67% (21/31)\u001b[K\rremote: Compressing objects:  70% (22/31)\u001b[K\rremote: Compressing objects:  74% (23/31)\u001b[K\rremote: Compressing objects:  77% (24/31)\u001b[K\rremote: Compressing objects:  80% (25/31)\u001b[K\rremote: Compressing objects:  83% (26/31)\u001b[K\rremote: Compressing objects:  87% (27/31)\u001b[K\rremote: Compressing objects:  90% (28/31)\u001b[K\rremote: Compressing objects:  93% (29/31)\u001b[K\rremote: Compressing objects:  96% (30/31)\u001b[K\rremote: Compressing objects: 100% (31/31)\u001b[K\rremote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "Unpacking objects:   2% (1/36)   \rUnpacking objects:   5% (2/36)   \rUnpacking objects:   8% (3/36)   \rUnpacking objects:  11% (4/36)   \rUnpacking objects:  13% (5/36)   \rUnpacking objects:  16% (6/36)   \rUnpacking objects:  19% (7/36)   \rUnpacking objects:  22% (8/36)   \rUnpacking objects:  25% (9/36)   \rUnpacking objects:  27% (10/36)   \rUnpacking objects:  30% (11/36)   \rUnpacking objects:  33% (12/36)   \rUnpacking objects:  36% (13/36)   \rUnpacking objects:  38% (14/36)   \rremote: Total 36 (delta 2), reused 28 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects:  41% (15/36)   \rUnpacking objects:  44% (16/36)   \rUnpacking objects:  47% (17/36)   \rUnpacking objects:  50% (18/36)   \rUnpacking objects:  52% (19/36)   \rUnpacking objects:  55% (20/36)   \rUnpacking objects:  58% (21/36)   \rUnpacking objects:  61% (22/36)   \rUnpacking objects:  63% (23/36)   \rUnpacking objects:  66% (24/36)   \rUnpacking objects:  69% (25/36)   \rUnpacking objects:  72% (26/36)   \rUnpacking objects:  75% (27/36)   \rUnpacking objects:  77% (28/36)   \rUnpacking objects:  80% (29/36)   \rUnpacking objects:  83% (30/36)   \rUnpacking objects:  86% (31/36)   \rUnpacking objects:  88% (32/36)   \rUnpacking objects:  91% (33/36)   \rUnpacking objects:  94% (34/36)   \rUnpacking objects:  97% (35/36)   \rUnpacking objects: 100% (36/36)   \rUnpacking objects: 100% (36/36), done.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: py-rouge in /usr/local/lib/python3.6/dist-packages (1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoWqnJwhO7Ls",
        "colab_type": "text"
      },
      "source": [
        "### GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwRRhY4zO8fy",
        "colab_type": "code",
        "outputId": "6fc0621d-46be-445f-9d0e-96ff94542ca4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained BERTABS fine-tuned on CNN/DM corpus \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if INLINE_MODE:\n",
        "  # from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "  # from transformers import AlbertConfig, AlbertForSequenceClassification\n",
        "  # import torch\n",
        "  from modeling_bertabs import BertAbs, build_predictor\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "  model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n",
        "  model.to(args.device)\n",
        "  model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffzmDA9tYXb",
        "colab_type": "text"
      },
      "source": [
        "### Text to be summarized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZPkjCsteSI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b10d4fc5-faf5-4f3c-e264-b9574199a310"
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  DATA_PATH=\"/content/textdata\"\n",
        "\n",
        "  !mkdir -p $DATA_PATH\n",
        "  !ls -al $DATA_PATH\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jun  9 20:18 .\n",
            "drwxr-xr-x 1 root root 4096 Jun  9 20:57 ..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mPiMrR_e5-RE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1aa97ca2-5eea-443e-cd21-76a07fe6dc7f"
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !cd nlp-qa/textsum/bertabs && echo $DATA_PATH"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/textdata\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34KA-DEywd1",
        "colab_type": "code",
        "outputId": "6b0c9df4-a394-40d1-84aa-a7518ed6dcec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!cd nlp-qa/textsum/bertabs && echo $DATA_PATH\n",
        "if EXAMPLE_MODE:\n",
        "  # !pwd\n",
        "  # !ls -al transformers/examples/summarization/bertabs\n",
        "  # !cd transformers/examples/summarization/bertabs && python run_summarization.py \\\n",
        "  #   --documents_dir $DATA_PATH \\\n",
        "  #   --summaries_output_dir $SUMMARIES_PATH \\\n",
        "  #   --no_cuda false \\\n",
        "  #   --batch_size 4 \\\n",
        "  #   --min_length 50 \\\n",
        "  #   --max_length 200 \\\n",
        "  #   --beam_size 5 \\\n",
        "  #   --alpha 0.95 \\\n",
        "  #   --block_trigram true\n",
        "  # !ls -al nlp-qa/textsum/bertabs\n",
        "  !cd nlp-qa/textsum/bertabs && echo $DATA_PATH && python run_summarization.py --documents_dir $DATA_PATH \\\n",
        "    --summaries_output_dir $SUMMARIES_PATH \\\n",
        "    --no_cuda false \\\n",
        "    --batch_size 4 \\\n",
        "    --min_length 50 \\\n",
        "    --max_length 200 \\\n",
        "    --beam_size 5 \\\n",
        "    --alpha 0.95 \\\n",
        "    --block_trigram true"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/textdata\n",
            "\n",
            "2020-06-09 21:10:17.471298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "usage: run_summarization.py [-h] --documents_dir DOCUMENTS_DIR\n",
            "                            [--summaries_output_dir SUMMARIES_OUTPUT_DIR]\n",
            "                            [--compute_rouge COMPUTE_ROUGE]\n",
            "                            [--no_cuda NO_CUDA] [--batch_size BATCH_SIZE]\n",
            "                            [--min_length MIN_LENGTH]\n",
            "                            [--max_length MAX_LENGTH] [--beam_size BEAM_SIZE]\n",
            "                            [--alpha ALPHA] [--block_trigram BLOCK_TRIGRAM]\n",
            "run_summarization.py: error: argument --documents_dir: expected one argument\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}