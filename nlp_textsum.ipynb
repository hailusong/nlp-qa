{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-textsum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPLUc1FjR/cr3dlMKVl6WD0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_textsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "#### **Text Summarization**\n",
        "Hyper-params:\n",
        "TBA\n",
        "\n",
        "Issues:\n",
        "1. Max sentence length: 64 (truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of Huggingface transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "a9faf861-c561-4e36-dc38-36deaae6e679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!uname -a\n",
        "!pip install wget\n",
        "!apt-get install file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 9af9b4b57846 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "file is already the newest version (1:5.32-2ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "dc966ff7-2114-4004-eb8a-ae17b520c831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMf33lSSGpQ",
        "colab_type": "code",
        "outputId": "c50c6c52-c822-4ecc-fc22-c733db07108f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!ls -al\n",
        "# !rm -rf nlp-qa\n",
        "# !git clone --depth 1 https://github.com/hailusong/nlp-qa.git\n",
        "# !pip install -r nlp-qa/question-answering/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 272\n",
            "drwxr-xr-x 1 root root   4096 Jun  7 14:55 .\n",
            "drwxr-xr-x 1 root root   4096 Jun  7 14:26 ..\n",
            "drwxr-xr-x 4 root root   4096 Dec 14  2018 cola_public\n",
            "-rw-r--r-- 1 root root 255330 Jun  7 14:35 cola_public_1.1.zip\n",
            "drwxr-xr-x 1 root root   4096 Jun  2 16:14 .config\n",
            "drwxr-xr-x 1 root root   4096 May 29 18:19 sample_data\n",
            "Cloning into 'nlp-qa'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLE_MODE=True\n",
        "INLINE_MODE=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "0d52684f-0c64-4927-841b-7b816437e029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !git clone --depth 1 https://github.com/huggingface/transformers && cd transformers\n",
        "  !pip install transformers/.\n",
        "  !pip install nltk py-rouge"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 1055, done.\u001b[K\n",
            "remote: Counting objects: 100% (1055/1055), done.\u001b[K\n",
            "remote: Compressing objects: 100% (687/687), done.\u001b[K\n",
            "remote: Total 1055 (delta 187), reused 660 (delta 105), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1055/1055), 3.27 MiB | 15.94 MiB/s, done.\n",
            "Resolving deltas: 100% (187/187), done.\n",
            "Processing ./transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (1.18.4)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 2.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 23.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 31.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from transformers==2.11.0) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==2.11.0) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.11.0) (2020.4.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.11.0) (0.15.1)\n",
            "Building wheels for collected packages: transformers, sacremoses\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.11.0-cp36-none-any.whl size=675627 sha256=f4a4c60d5abfe7c7f314984f63cb4fc33c4f301429c02e5b1dcea0c4941b6e5e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cxezj6ac/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=271d9fc3f3c3dd3e7970b1dabf69fc238a9439d84dcc5effe091013a4a5751bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built transformers sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.11.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Collecting py-rouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Installing collected packages: py-rouge\n",
            "Successfully installed py-rouge-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoWqnJwhO7Ls",
        "colab_type": "text"
      },
      "source": [
        "### GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwRRhY4zO8fy",
        "colab_type": "code",
        "outputId": "8511b66d-8bcc-4d64-c358-d8b5e8ee2619",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained BERTABS fine-tuned on CNN/DM corpus \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if INLINE_MODE:\n",
        "  # from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "  # from transformers import AlbertConfig, AlbertForSequenceClassification\n",
        "  # import torch\n",
        "  from modeling_bertabs import BertAbs, build_predictor\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "  model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n",
        "  model.to(args.device)\n",
        "  model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffzmDA9tYXb",
        "colab_type": "text"
      },
      "source": [
        "### Text to be summarized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZPkjCsteSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  DATA_PATH=./textdata\n",
        "\n",
        "  !mkdir -p $DATA_PATH\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34KA-DEywd1",
        "colab_type": "code",
        "outputId": "c7a11ae9-603f-4749-94e3-c68d86e505b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !python run_summarization.py \\\n",
        "      --documents_dir $DATA_PATH \\\n",
        "      --summaries_output_dir $SUMMARIES_PATH \\ # optional\n",
        "      --no_cuda false \\\n",
        "      --batch_size 4 \\\n",
        "      --min_length 50 \\\n",
        "      --max_length 200 \\\n",
        "      --beam_size 5 \\\n",
        "      --alpha 0.95 \\\n",
        "      --block_trigram true \\"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-56af27eadc65>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    --no_cuda false       --batch_size 4       --min_length 50       --max_length 200       --beam_size 5       --alpha 0.95       --block_trigram true\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}