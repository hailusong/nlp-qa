{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-textsum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0RuP83UrIo+PcVGgXHocg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "367fa278fb33466b979d32c5b3d1679a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f15865ccdf1435bbd01222876225e7d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ff0b602292654552ba29f1498b38c0fc",
              "IPY_MODEL_9756e6092e67454aa02873458c871138"
            ]
          }
        },
        "7f15865ccdf1435bbd01222876225e7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ff0b602292654552ba29f1498b38c0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_267259b3b21c4971a6d0127b48d011f6",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 760289,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 760289,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb3ea25ff9934dbcb2e1d6cd3f72ecb5"
          }
        },
        "9756e6092e67454aa02873458c871138": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9e4f15e6d896439e9db93d9adad56e23",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 760k/760k [00:02&lt;00:00, 375kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1d0d6bc2e5f8434c963ca6963033ca30"
          }
        },
        "267259b3b21c4971a6d0127b48d011f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb3ea25ff9934dbcb2e1d6cd3f72ecb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e4f15e6d896439e9db93d9adad56e23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1d0d6bc2e5f8434c963ca6963033ca30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_textsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "#### **Text Summarization**\n",
        "Hyper-params:\n",
        "TBA\n",
        "\n",
        "Issues:\n",
        "1. Max sentence length: 64 (truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of Huggingface transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "a9faf861-c561-4e36-dc38-36deaae6e679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!uname -a\n",
        "!pip install wget\n",
        "!apt-get install file"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 9af9b4b57846 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "file is already the newest version (1:5.32-2ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "dc966ff7-2114-4004-eb8a-ae17b520c831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AMf33lSSGpQ",
        "colab_type": "code",
        "outputId": "c50c6c52-c822-4ecc-fc22-c733db07108f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!ls -al\n",
        "# !rm -rf nlp-qa\n",
        "# !git clone --depth 1 https://github.com/hailusong/nlp-qa.git\n",
        "# !pip install -r nlp-qa/question-answering/requirements.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 272\n",
            "drwxr-xr-x 1 root root   4096 Jun  7 14:55 .\n",
            "drwxr-xr-x 1 root root   4096 Jun  7 14:26 ..\n",
            "drwxr-xr-x 4 root root   4096 Dec 14  2018 cola_public\n",
            "-rw-r--r-- 1 root root 255330 Jun  7 14:35 cola_public_1.1.zip\n",
            "drwxr-xr-x 1 root root   4096 Jun  2 16:14 .config\n",
            "drwxr-xr-x 1 root root   4096 May 29 18:19 sample_data\n",
            "Cloning into 'nlp-qa'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLE_MODE=True\n",
        "INLINE_MODE=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "09399b7e-c82e-4f3d-f7af-7a9fcc1e9e99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !git clone --depth 1 https://github.com/huggingface/transformers && cd transformers\n",
        "  !pip install transformers/.\n",
        "  !pip install nltk py-rouge"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'nlp-qa/question-answering/requirements.txt'\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoWqnJwhO7Ls",
        "colab_type": "text"
      },
      "source": [
        "### GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwRRhY4zO8fy",
        "colab_type": "code",
        "outputId": "3ce3cdc8-0297-4b83-ab49-210d05ee253f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained BERTABS fine-tuned on CNN/DM corpus \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "outputId": "3c84b95e-5ef3-4169-fb30-b4d191c6d1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627,
          "referenced_widgets": [
            "367fa278fb33466b979d32c5b3d1679a",
            "7f15865ccdf1435bbd01222876225e7d",
            "ff0b602292654552ba29f1498b38c0fc",
            "9756e6092e67454aa02873458c871138",
            "267259b3b21c4971a6d0127b48d011f6",
            "eb3ea25ff9934dbcb2e1d6cd3f72ecb5",
            "9e4f15e6d896439e9db93d9adad56e23",
            "1d0d6bc2e5f8434c963ca6963033ca30"
          ]
        }
      },
      "source": [
        "if INLINE_MODE:\n",
        "  # from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "  # from transformers import AlbertConfig, AlbertForSequenceClassification\n",
        "  # import torch\n",
        "  from modeling_bertabs import BertAbs, build_predictor\n",
        "  from transformers import BertTokenizer\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "  model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n",
        "  model.to(args.device)\n",
        "  model.eval()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "AlbertConfig {\n",
            "  \"architectures\": [\n",
            "    \"AlbertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0,\n",
            "  \"bos_token_id\": 2,\n",
            "  \"classifier_dropout_prob\": 0.1,\n",
            "  \"down_scale_factor\": 1,\n",
            "  \"embedding_size\": 128,\n",
            "  \"eos_token_id\": 3,\n",
            "  \"gap_size\": 0,\n",
            "  \"hidden_act\": \"gelu_new\",\n",
            "  \"hidden_dropout_prob\": 0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"inner_group_num\": 1,\n",
            "  \"intermediate_size\": 4096,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"albert\",\n",
            "  \"net_structure_type\": 0,\n",
            "  \"num_attention_heads\": 16,\n",
            "  \"num_hidden_groups\": 1,\n",
            "  \"num_hidden_layers\": 24,\n",
            "  \"num_memory_blocks\": 0,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30000\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n",
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "367fa278fb33466b979d32c5b3d1679a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760289.0, style=ProgressStyle(descriptiâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34KA-DEywd1",
        "colab_type": "code",
        "outputId": "08a404b1-0a70-4252-f486-fb797c21e51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !python run_summarization.py \\\n",
        "      --documents_dir $DATA_PATH \\\n",
        "      --summaries_output_dir $SUMMARIES_PATH \\ # optional\n",
        "      --no_cuda false \\\n",
        "      --batch_size 4 \\\n",
        "      --min_length 50 \\\n",
        "      --max_length 200 \\\n",
        "      --beam_size 5 \\\n",
        "      --alpha 0.95 \\\n",
        "      --block_trigram true \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "albert.pooler.weight                                    (1024, 1024)\n",
            "albert.pooler.bias                                           (1024,)\n",
            "classifier.weight                                          (2, 1024)\n",
            "classifier.bias                                                 (2,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8v7q4FyjPnd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(args):\n",
        "\n",
        "    symbols = {\n",
        "        \"BOS\": tokenizer.vocab[\"[unused0]\"],\n",
        "        \"EOS\": tokenizer.vocab[\"[unused1]\"],\n",
        "        \"PAD\": tokenizer.vocab[\"[PAD]\"],\n",
        "    }\n",
        "\n",
        "    if args.compute_rouge:\n",
        "        reference_summaries = []\n",
        "        generated_summaries = []\n",
        "\n",
        "        import rouge\n",
        "        import nltk\n",
        "\n",
        "        nltk.download(\"punkt\")\n",
        "        rouge_evaluator = rouge.Rouge(\n",
        "            metrics=[\"rouge-n\", \"rouge-l\"],\n",
        "            max_n=2,\n",
        "            limit_length=True,\n",
        "            length_limit=args.beam_size,\n",
        "            length_limit_type=\"words\",\n",
        "            apply_avg=True,\n",
        "            apply_best=False,\n",
        "            alpha=0.5,  # Default F1_score\n",
        "            weight_factor=1.2,\n",
        "            stemming=True,\n",
        "        )\n",
        "\n",
        "    # these (unused) arguments are defined to keep the compatibility\n",
        "    # with the legacy code and will be deleted in a next iteration.\n",
        "    args.result_path = \"\"\n",
        "    args.temp_dir = \"\"\n",
        "\n",
        "    data_iterator = build_data_iterator(args, tokenizer)\n",
        "    predictor = build_predictor(args, tokenizer, symbols, model)\n",
        "\n",
        "    logger.info(\"***** Running evaluation *****\")\n",
        "    logger.info(\"  Number examples = %d\", len(data_iterator.dataset))\n",
        "    logger.info(\"  Batch size = %d\", args.batch_size)\n",
        "    logger.info(\"\")\n",
        "    logger.info(\"***** Beam Search parameters *****\")\n",
        "    logger.info(\"  Beam size = %d\", args.beam_size)\n",
        "    logger.info(\"  Minimum length = %d\", args.min_length)\n",
        "    logger.info(\"  Maximum length = %d\", args.max_length)\n",
        "    logger.info(\"  Alpha (length penalty) = %.2f\", args.alpha)\n",
        "    logger.info(\"  Trigrams %s be blocked\", (\"will\" if args.block_trigram else \"will NOT\"))\n",
        "\n",
        "    for batch in tqdm(data_iterator):\n",
        "        batch_data = predictor.translate_batch(batch)\n",
        "        translations = predictor.from_batch(batch_data)\n",
        "        summaries = [format_summary(t) for t in translations]\n",
        "        save_summaries(summaries, args.summaries_output_dir, batch.document_names)\n",
        "\n",
        "        if args.compute_rouge:\n",
        "            reference_summaries += batch.tgt_str\n",
        "            generated_summaries += summaries\n",
        "\n",
        "    if args.compute_rouge:\n",
        "        scores = rouge_evaluator.get_scores(generated_summaries, reference_summaries)\n",
        "        str_scores = format_rouge_scores(scores)\n",
        "        save_rouge_scores(str_scores)\n",
        "        print(str_scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ghMbXVpFKhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if QA_EXTRACTION:\n",
        "  # model = torch.hub.load('huggingface/pytorch-transformers', 'modelForQuestionAnswering', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "  # tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "\n",
        "  # The format is paragraph first and then question\n",
        "  # fact = \"Jim Henson was a puppeteer\"\n",
        "  # q = \"Who was Jim Henson ?\"\n",
        "  fact = input('Fact: ')\n",
        "  q = input('Question: ')\n",
        "\n",
        "  # Converts a list of string to a sequence of ids (integer), using the tokenizer and vocabulary. \n",
        "  # Also request to add model-specific special tokens (such as beginning of sequence, end of sequence, sequence separator).\n",
        "  indexed_tokens_fact = tokenizer.encode(fact, add_special_tokens=True)\n",
        "  indexed_tokens_q = tokenizer.encode(q, add_special_tokens=True)\n",
        "\n",
        "  # merge fact tokens and Q tokens by removing the START token in Q tokens\n",
        "  indexed_tokens = indexed_tokens_fact + indexed_tokens_q[1:]\n",
        "  segments_ids = [0] * len(indexed_tokens_fact) + [1] * len(indexed_tokens_q[1:])\n",
        "\n",
        "  print(f'Length is {len(indexed_tokens)}, Indexed token: {indexed_tokens}')\n",
        "\n",
        "  # Predict the start and end positions logits\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "  with torch.no_grad():\n",
        "      start_logits, end_logits = model(tokens_tensor, token_type_ids=segments_tensors)\n",
        "\n",
        "  # get the highest prediction\n",
        "  answer = tokenizer.decode(indexed_tokens[torch.argmax(start_logits):torch.argmax(end_logits)+1])\n",
        "  print(f'The answer is: {answer}')\n",
        "\n",
        "  # Or get the total loss which is the sum of the CrossEntropy loss for the start and end token positions (set model to train mode before if used for training)\n",
        "  # start_positions, end_positions = torch.tensor([12]), torch.tensor([14])\n",
        "  # multiple_choice_loss = model(tokens_tensor, token_type_ids=segments_tensors, start_positions=start_positions, end_positions=end_positions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}