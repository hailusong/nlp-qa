{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-textsum.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8ppoyVj3eiArTQwtKix28",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hailusong/nlp-qa/blob/master/nlp_textsum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-mZgVZMlbDh",
        "colab_type": "text"
      },
      "source": [
        "#### **Text Summarization**\n",
        "Refs:\n",
        "- https://huggingface.co/models\n",
        "- https://huggingface.co/models?search=bertabs\n",
        "- [BertForMaskedLM](https://github.com/huggingface/transformers/blob/13aa174112f0c2ee794c44188ecf13b241694db0/src/transformers/modeling_bert.py#L876) used as architecture in BertAbs model, which can do\n",
        "  * **masked language modeling** loss, for language modeling, or\n",
        "  * left-to-right language modeling loss (**next word prediction**)\n",
        "\n",
        "Hyper-params:\n",
        "TBA\n",
        "\n",
        "Issues:\n",
        "1. Max sentence length: 64 (truncated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nK4Aexd2YLi9",
        "colab_type": "text"
      },
      "source": [
        "## Environment Setup\n",
        "To get the source code of Huggingface transformer release, do this (assuming the release version is 2.11.0)\n",
        "```\n",
        "git fetch -a --tags\n",
        "git checkout tags/v2.11.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYM3yVozc9uj",
        "colab_type": "text"
      },
      "source": [
        "Make sure we have access to file command in Linux"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "799C3kzOcqb4",
        "colab_type": "code",
        "outputId": "b378786a-186d-46fa-968a-0718703a535f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!uname -a\n",
        "!pip install wget\n",
        "!apt-get install file"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linux 153b72610239 4.19.104+ #1 SMP Wed Feb 19 05:26:34 PST 2020 x86_64 x86_64 x86_64 GNU/Linux\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "file is already the newest version (1:5.32-2ubuntu0.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 32 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpUI_vQISUii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import transformers\n",
        "except ImportError as e:\n",
        "    # install huggingface\n",
        "    !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Guw8jy8IRy3Q",
        "colab_type": "code",
        "outputId": "1278fe80-7b36-4646-e12a-59817bdb8017",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "print(torch.__version__)\n",
        "print(transformers.__version__)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n",
            "2.11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcrZk3mrW8F",
        "colab_type": "text"
      },
      "source": [
        "## Configuratiuon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QPgyarrrY_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EXAMPLE_MODE=True\n",
        "INLINE_MODE=True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzR-3Gj3fgVZ",
        "colab_type": "code",
        "outputId": "7a2ff338-1343-4bd6-d637-af2132e587d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "if EXAMPLE_MODE or INLINE_MODE:\n",
        "  # ! [ ! -d \"./transformers\" ] && git clone --depth 1 https://github.com/huggingface/transformers\n",
        "  # ! [ -d \"./transformers\" ] && \\\n",
        "  #   cd transformers && \\\n",
        "  #   git fetch -a --tags && \\\n",
        "  #   git checkout tags/v2.11.0 && \\\n",
        "  #   pip install . && \\\n",
        "  !rm -rf ./nlp-qa/\n",
        "  ![ ! -d \"./nlp-qa\" ] && git clone --depth 1 https://github.com/hailusong/nlp-qa.git\n",
        "  !pip install nltk py-rouge"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-qa'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 45 (delta 3), reused 28 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: py-rouge in /usr/local/lib/python3.6/dist-packages (1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoWqnJwhO7Ls",
        "colab_type": "text"
      },
      "source": [
        "### GPU Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwRRhY4zO8fy",
        "colab_type": "code",
        "outputId": "ae347654-2b71-4947-f379-81494452d21e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ1AUfsqlwa0",
        "colab_type": "text"
      },
      "source": [
        "## Download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WSQ93jDoh14",
        "colab_type": "text"
      },
      "source": [
        "### Pre-trained BERTABS fine-tuned on CNN/DM corpus \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exGaF-bRoRCh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if INLINE_MODE:\n",
        "#   # from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "#   # from transformers import AlbertConfig, AlbertForSequenceClassification\n",
        "#   # import torch\n",
        "#   from modeling_bertabs import BertAbs, build_predictor\n",
        "#   from transformers import BertTokenizer\n",
        "\n",
        "#   tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
        "#   model = BertAbs.from_pretrained(\"bertabs-finetuned-cnndm\")\n",
        "#   model.to(args.device)\n",
        "#   model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ffzmDA9tYXb",
        "colab_type": "text"
      },
      "source": [
        "### Text to be summarized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZPkjCsteSI",
        "colab_type": "code",
        "outputId": "b4aa1281-62b9-46ff-f4cb-6de402ad11d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  %env DATA_PATH=/content/nlp-qa/stories\n",
        "  %env SUMMARIES_PATH=/content/textsum-out\n",
        "\n",
        "  !mkdir -p $SUMMARIES_PATH\n",
        "  !ls -al $SUMMARIES_PATH\n",
        "\n",
        "if INLINE_MODE:\n",
        "  DATA_PATH='/content/nlp-qa/stories'\n",
        "  SUMMARIES_PATH='/content/textsum-out'\n",
        "\n",
        "  !mkdir -p $SUMMARIES_PATH\n",
        "  !ls -al $SUMMARIES_PATH\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: DATA_PATH=/content/nlp-qa/stories\n",
            "env: SUMMARIES_PATH=/content/textsum-out\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jun 10 18:46 .\n",
            "drwxr-xr-x 1 root root 4096 Jun 10 18:55 ..\n",
            "-rw-r--r-- 1 root root    0 Jun 10 18:53 bart_cnn_test_summaries.txt\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Jun 10 18:46 .\n",
            "drwxr-xr-x 1 root root 4096 Jun 10 18:55 ..\n",
            "-rw-r--r-- 1 root root    0 Jun 10 18:53 bart_cnn_test_summaries.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxUPF8mHEuEq",
        "colab_type": "text"
      },
      "source": [
        "## Inference\n",
        "Source: https://pytorch.org/hub/huggingface_pytorch-transformers/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GMmocddvlOw",
        "colab_type": "text"
      },
      "source": [
        "### BertAbs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v34KA-DEywd1",
        "colab_type": "code",
        "outputId": "f60f2838-e0dd-442b-b256-f04c513d1144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  # !pwd\n",
        "  # !ls -al transformers/examples/summarization/bertabs\n",
        "  # !cd transformers/examples/summarization/bertabs && python run_summarization.py \\\n",
        "  #   --documents_dir $DATA_PATH \\\n",
        "  #   --summaries_output_dir $SUMMARIES_PATH \\\n",
        "  #   --no_cuda false \\\n",
        "  #   --batch_size 4 \\\n",
        "  #   --min_length 50 \\\n",
        "  #   --max_length 200 \\\n",
        "  #   --beam_size 5 \\\n",
        "  #   --alpha 0.95 \\\n",
        "  #   --block_trigram true\n",
        "  # !ls -al nlp-qa/textsum/bertabs\n",
        "  !cd nlp-qa/textsum/bertabs && python run_summarization.py \\\n",
        "    --documents_dir $DATA_PATH \\\n",
        "    --summaries_output_dir $SUMMARIES_PATH \\\n",
        "    --no_cuda false \\\n",
        "    --batch_size 4 \\\n",
        "    --min_length 20 \\\n",
        "    --max_length 50 \\\n",
        "    --beam_size 5 \\\n",
        "    --alpha 0.95 \\\n",
        "    --block_trigram true"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-06-10 18:55:53.511620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "INFO:filelock:Lock 139990310497304 acquired on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpuksi2uzg\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 856kB/s] \n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:filelock:Lock 139990310497304 released on /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084.lock\n",
            "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "INFO:filelock:Lock 139989935596040 acquired on /root/.cache/torch/transformers/9d982039993bba60a064b70f3d5733f091aefc83eef2f1cd222242ca2f4fd0d9.d524a81afda9167a3e76519785c0610c76f5647e45b9c17a8849f7fe0088cf5f.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpl7fe_5as\n",
            "Downloading: 100% 666/666 [00:00<00:00, 445kB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/config.json in cache at /root/.cache/torch/transformers/9d982039993bba60a064b70f3d5733f091aefc83eef2f1cd222242ca2f4fd0d9.d524a81afda9167a3e76519785c0610c76f5647e45b9c17a8849f7fe0088cf5f\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/9d982039993bba60a064b70f3d5733f091aefc83eef2f1cd222242ca2f4fd0d9.d524a81afda9167a3e76519785c0610c76f5647e45b9c17a8849f7fe0088cf5f\n",
            "INFO:filelock:Lock 139989935596040 released on /root/.cache/torch/transformers/9d982039993bba60a064b70f3d5733f091aefc83eef2f1cd222242ca2f4fd0d9.d524a81afda9167a3e76519785c0610c76f5647e45b9c17a8849f7fe0088cf5f.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/config.json from cache at /root/.cache/torch/transformers/9d982039993bba60a064b70f3d5733f091aefc83eef2f1cd222242ca2f4fd0d9.d524a81afda9167a3e76519785c0610c76f5647e45b9c17a8849f7fe0088cf5f\n",
            "INFO:transformers.configuration_utils:Model config BertAbsConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"dec_dropout\": 0.2,\n",
            "  \"dec_ff_size\": 2048,\n",
            "  \"dec_heads\": 8,\n",
            "  \"dec_hidden_size\": 768,\n",
            "  \"dec_layers\": 6,\n",
            "  \"enc_dropout\": 0.2,\n",
            "  \"enc_ff_size\": 512,\n",
            "  \"enc_heads\": 8,\n",
            "  \"enc_hidden_size\": 512,\n",
            "  \"enc_layers\": 6,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_pos\": 512,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bertabs\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:filelock:Lock 139989789216896 acquired on /root/.cache/torch/transformers/a8eb33263409540b0986df2844c815c973e453432ff22805e5ed0c1ec585d98c.f7b70c5121ce72aad107e20f528d8fd88aa9da525c556aeadeab0c0421696223.lock\n",
            "INFO:transformers.file_utils:https://cdn.huggingface.co/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp4_a93kmy\n",
            "Downloading: 100% 886M/886M [02:02<00:00, 7.24MB/s]\n",
            "INFO:transformers.file_utils:storing https://cdn.huggingface.co/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin in cache at /root/.cache/torch/transformers/a8eb33263409540b0986df2844c815c973e453432ff22805e5ed0c1ec585d98c.f7b70c5121ce72aad107e20f528d8fd88aa9da525c556aeadeab0c0421696223\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/a8eb33263409540b0986df2844c815c973e453432ff22805e5ed0c1ec585d98c.f7b70c5121ce72aad107e20f528d8fd88aa9da525c556aeadeab0c0421696223\n",
            "INFO:filelock:Lock 139989789216896 released on /root/.cache/torch/transformers/a8eb33263409540b0986df2844c815c973e453432ff22805e5ed0c1ec585d98c.f7b70c5121ce72aad107e20f528d8fd88aa9da525c556aeadeab0c0421696223.lock\n",
            "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/remi/bertabs-finetuned-cnndm-extractive-abstractive-summarization/pytorch_model.bin from cache at /root/.cache/torch/transformers/a8eb33263409540b0986df2844c815c973e453432ff22805e5ed0c1ec585d98c.f7b70c5121ce72aad107e20f528d8fd88aa9da525c556aeadeab0c0421696223\n",
            "INFO:filelock:Lock 139989936036104 acquired on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpug9ljdvx\n",
            "Downloading: 100% 433/433 [00:00<00:00, 259kB/s]\n",
            "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:filelock:Lock 139989936036104 released on /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517.lock\n",
            "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
            "INFO:transformers.configuration_utils:Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "INFO:__main__:***** Running evaluation *****\n",
            "INFO:__main__:  Number examples = 8\n",
            "INFO:__main__:  Batch size = 4\n",
            "INFO:__main__:\n",
            "INFO:__main__:***** Beam Search parameters *****\n",
            "INFO:__main__:  Beam size = 5\n",
            "INFO:__main__:  Minimum length = 20\n",
            "INFO:__main__:  Maximum length = 50\n",
            "INFO:__main__:  Alpha (length penalty) = 0.95\n",
            "INFO:__main__:  Trigrams will be blocked\n",
            "  0% 0/2 [00:00<?, ?it/s]/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n",
            "100% 2/2 [00:52<00:00, 26.49s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9mTlxoRvpJU",
        "colab_type": "text"
      },
      "source": [
        "### BART"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC-0O7nIJ-SJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "969ac6f2-290e-44d8-f7da-f19b4b3de8b0"
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !ls -al nlp-qa/textsum/bart\n",
        "  !cd nlp-qa/textsum/bart && python evaluate_cnn.py $DATA_PATH/mine1.story $SUMMARIES_PATH/bart_cnn_test_summaries.txt facebook/bart-large-cnn"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 44\n",
            "drwxr-xr-x 2 root root 4096 Jun 10 18:55 .\n",
            "drwxr-xr-x 5 root root 4096 Jun 10 18:55 ..\n",
            "-rw-r--r-- 1 root root 2553 Jun 10 18:55 evaluate_cnn.py\n",
            "-rw-r--r-- 1 root root 7175 Jun 10 18:55 finetune.py\n",
            "-rw-r--r-- 1 root root    0 Jun 10 18:55 __init__.py\n",
            "-rw-r--r-- 1 root root 1948 Jun 10 18:55 README.md\n",
            "-rwxr-xr-x 1 root root  488 Jun 10 18:55 run_train.sh\n",
            "-rwxr-xr-x 1 root root  875 Jun 10 18:55 run_train_tiny.sh\n",
            "-rw-r--r-- 1 root root 5449 Jun 10 18:55 test_bart_examples.py\n",
            "-rw-r--r-- 1 root root 2229 Jun 10 18:55 utils.py\n",
            "2020-06-10 18:59:05.608591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "100% 1/1 [00:02<00:00,  2.72s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by-nlDGNioOw",
        "colab_type": "code",
        "outputId": "f3efe380-8293-49af-c8e7-11f40e756f72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "if EXAMPLE_MODE:\n",
        "  !ls -al $SUMMARIES_PATH\n",
        "  !wc -l $SUMMARIES_PATH/*\n",
        "  !grep \"\" $SUMMARIES_PATH/*"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 44\n",
            "drwxr-xr-x 2 root root 4096 Jun 10 18:59 .\n",
            "drwxr-xr-x 1 root root 4096 Jun 10 18:55 ..\n",
            "-rw-r--r-- 1 root root  683 Jun 10 18:59 bart_cnn_test_summaries.txt\n",
            "-rw-r--r-- 1 root root  206 Jun 10 18:59 e117408ad19cc69e15b1e21b9ae54f10c07223ce_summary.story\n",
            "-rw-r--r-- 1 root root  252 Jun 10 18:58 e3dd06d326c8d53722bdc5c8428e29c68a60d0d5_summary.story\n",
            "-rw-r--r-- 1 root root  228 Jun 10 18:58 e9093ca0a82f2aba28cd2762942c04177cfbb000_summary.story\n",
            "-rw-r--r-- 1 root root  198 Jun 10 18:58 ea06fd0b25cb9793397a51de73fd83f91b4323fa_summary.story\n",
            "-rw-r--r-- 1 root root  243 Jun 10 18:59 eb68bc51ed4fc727a1af058192a3fef0916c91e7_summary.story\n",
            "-rw-r--r-- 1 root root  267 Jun 10 18:58 ee0ba7928d8987f2cb21c9a2012a76730f77de45_summary.story\n",
            "-rw-r--r-- 1 root root  218 Jun 10 18:59 ee8871b15c50d0db17b0179a6d2beab35065f1e9_summary.story\n",
            "-rw-r--r-- 1 root root  228 Jun 10 18:59 mine1_summary.story\n",
            "   2 /content/textsum-out/bart_cnn_test_summaries.txt\n",
            "   0 /content/textsum-out/e117408ad19cc69e15b1e21b9ae54f10c07223ce_summary.story\n",
            "   0 /content/textsum-out/e3dd06d326c8d53722bdc5c8428e29c68a60d0d5_summary.story\n",
            "   0 /content/textsum-out/e9093ca0a82f2aba28cd2762942c04177cfbb000_summary.story\n",
            "   0 /content/textsum-out/ea06fd0b25cb9793397a51de73fd83f91b4323fa_summary.story\n",
            "   0 /content/textsum-out/eb68bc51ed4fc727a1af058192a3fef0916c91e7_summary.story\n",
            "   0 /content/textsum-out/ee0ba7928d8987f2cb21c9a2012a76730f77de45_summary.story\n",
            "   0 /content/textsum-out/ee8871b15c50d0db17b0179a6d2beab35065f1e9_summary.story\n",
            "   0 /content/textsum-out/mine1_summary.story\n",
            "   2 total\n",
            "/content/textsum-out/bart_cnn_test_summaries.txt:Clear Living Clinic will be limiting the number of clients in the clinic at all times. Non-clients will be asked to kindly wait outside the clinic if someone is accompanying you to your appointment. If you have traveled recently, feel under the weather or are taking care of someone who is ill, please call your family doctor or Telehealth Ontario and self-isolate.\n",
            "/content/textsum-out/bart_cnn_test_summaries.txt:Please wash your hands upon arrival. As usual, we provide you with hand sanitizer and paper towels. Social distancing guidelines will be honored between staff and clients. With our Angel of Water colonic equipment, social distancing is easy. We highly recommend everyone to wear a mask or some form of face covering.\n",
            "/content/textsum-out/e117408ad19cc69e15b1e21b9ae54f10c07223ce_summary.story:the partnership is an organization with partners throughout the uk. it employs approximately 67 , 100 people. each partner receives the same scale of bonus , based on a fixed percentage of their annual wage\n",
            "/content/textsum-out/e3dd06d326c8d53722bdc5c8428e29c68a60d0d5_summary.story:six south carolina students were able to get out of the house in time. six others were treated and released from hospital , but auman 's daughter is in stable condition. the fire department arrived on the scene , about five minutes after being notified\n",
            "/content/textsum-out/e9093ca0a82f2aba28cd2762942c04177cfbb000_summary.story:cpl. trent d. thomas was found guilty of kidnapping and conspiracy to commit several offenses. thomas was among seven marines and a navy medic who were charged in connection with the death of hashim ibrahim awad. the 25-year-old\n",
            "/content/textsum-out/ea06fd0b25cb9793397a51de73fd83f91b4323fa_summary.story:barcelona beat atletico madrid 3- 0-0 at the nou camp in el clasico. lionel messi scored twice in the first half for the spanish giants. the result leaves the catalan giants top of the spanish table\n",
            "/content/textsum-out/eb68bc51ed4fc727a1af058192a3fef0916c91e7_summary.story:honey bees are responsible for pollinating $ 15 billion worth of apples each year. more than 90 fruits and vegetables worldwide depend on them for pollination. the disorder is marked by hives left with a queen , a few newly hatched adults. but\n",
            "/content/textsum-out/ee0ba7928d8987f2cb21c9a2012a76730f77de45_summary.story:all of the unidentified victims - thai embassy officials - were foreign nationals. thai health minister mongkol na songkhla said that of the 41 survivors who remain hospitalized , 38 are in stable condition and three are still in critical condition. the investigation\n",
            "/content/textsum-out/ee8871b15c50d0db17b0179a6d2beab35065f1e9_summary.story:most often , they face drug charges or charges of assaulting an officer. inmates become more paranoid , delusional , and less likely to follow directions. judge steven leifman says most people in miami are mentally ill\n",
            "/content/textsum-out/mine1_summary.story:the clinic has been very proactive with disinfecting all surfaces using hospital grade disinfectant before and after each client. we are adding even more diligence to these protocols. we will provide you with disposable gloves ,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu1cPG1gWAuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if INLINE_MODE:\n",
        "  from pathlib import Path\n",
        "  from collections import namedtuple\n",
        "\n",
        "  import torch\n",
        "  from tqdm import tqdm\n",
        "\n",
        "  from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "\n",
        "  DEFAULT_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "  def chunks(lst, n):\n",
        "      \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "      for i in range(0, len(lst), n):\n",
        "          yield lst[i : i + n]\n",
        "\n",
        "\n",
        "  def generate_summaries(\n",
        "      examples: list, out_file: str, model_name: str, batch_size: int = 8, device: str = DEFAULT_DEVICE,\n",
        "      max_length = 140, min_length = 55\n",
        "  ):\n",
        "      fout = Path(out_file).open(\"w\")\n",
        "      model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "      tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
        "\n",
        "      for batch in tqdm(list(chunks(examples, batch_size))):\n",
        "          dct = tokenizer.batch_encode_plus(batch, max_length=1024, return_tensors=\"pt\", pad_to_max_length=True)\n",
        "          summaries = model.generate(\n",
        "              input_ids=dct[\"input_ids\"].to(device),\n",
        "              attention_mask=dct[\"attention_mask\"].to(device),\n",
        "              num_beams=4,\n",
        "              length_penalty=2.0,\n",
        "              max_length=max_length + 2,  # +2 from original because we start at step=1 and stop before max_length\n",
        "              min_length=min_length + 1,  # +1 from original because we start at step=1\n",
        "              no_repeat_ngram_size=3,\n",
        "              early_stopping=True,\n",
        "              decoder_start_token_id=model.config.eos_token_id,\n",
        "          )\n",
        "          dec = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summaries]\n",
        "          for hypothesis in dec:\n",
        "              # fout.write(hypothesis + \"\\n\")\n",
        "              # fout.flush()\n",
        "              print(\"\\n\" + hypothesis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iP70HlxTglvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f0ad819d-0857-455d-b791-cabcfc633e2f"
      },
      "source": [
        "if INLINE_MODE:\n",
        "    parser = namedtuple('parser', 'source_path output_path model_name device bs')\n",
        "    args = parser(\n",
        "        source_path = f'{DATA_PATH}/mine1.story',\n",
        "        output_path = f'{SUMMARIES_PATH}/bart_cnn_test_summaries.txt',\n",
        "        model_name = 'facebook/bart-large-cnn',\n",
        "        device = DEFAULT_DEVICE,\n",
        "        bs = 8)\n",
        "\n",
        "    examples = [\" \" + x.rstrip() for x in open(args.source_path).readlines()]\n",
        "    examples = [ \" \".join(examples) ]\n",
        "    print(f\"total lines: {len(examples)}, line data: {examples}\")\n",
        "    generate_summaries(examples, args.output_path, args.model_name, batch_size=args.bs, device=args.device, max_length=50, min_length=20)\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total lines: 1, line data: [' Here is our updated policy at Clear Living Clinic to keep you safe and provide peace-of-mind. We will be limiting the number of clients in the clinic at all times. We will be allowing only one client at a time in the waiting area. Non-clients will be asked to kindly wait outside the clinic if someone is accompanying you to your appointment. Upon booking an appointment, we will pre-screen every client before they are permitted to visit the clinic. If you have traveled recently, feel under the weather or are taking care of someone who is ill, please call your family doctor or Telehealth Ontario and self-isolate for 14 days.  Check-in: Please wash your hands upon arrival. As usual, we provide you with hand sanitizer and paper towels. We will also provide you with disposable gloves, a mask and a disposable toilet seat cover upon request. Our clinic has always been very proactive with disinfecting all surfaces using hospital grade disinfectant before and after each client. We are adding even more diligence to these protocols. We highly recommend everyone to wear a mask or some form of face covering. We can provide you with a mask if you don’t have one. Our therapists will be wearing a mask or face-shield. Social distancing guidelines will be honored between staff and clients. With our Angel of Water colonic equipment, social distancing is easy.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Clear Living Clinic will be limiting the number of clients in the clinic at all times. We will be allowing only one client at a time in the waiting area. Non-clients will be asked to kindly wait outside the clinic if someone is accompanying\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDvnKW-5fMFN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}